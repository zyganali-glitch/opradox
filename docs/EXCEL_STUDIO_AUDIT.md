# Excel Studio Audit - Risk HaritasÄ±

> **FAZ 0:** Read-only Audit Report  
> **Tarih:** 2026-01-08  
> **Kapsam:** BakÄ±m riskleri ve potansiyel sorunlar

---

## 1. In-Memory Store Riskleri

> [!CAUTION]  
> **KRÄ°TÄ°K RÄ°SK:** TÃ¼m in-memory store'lar server restart'ta kaybedilir. Production ortamÄ±nda Redis/Database geÃ§iÅŸi zorunludur.

### 1.1 LAST_EXCEL_STORE

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Dosya** | `backend/app/scenario_registry.py` |
| **TanÄ±m SatÄ±rÄ±** | Line 22 |
| **KullanÄ±m** | `main.py` lines 17, 248, 250, 375, 381, 478, 484 |

**Kod KanÄ±tÄ±:**
```python
# scenario_registry.py:22
LAST_EXCEL_STORE: Dict[str, Dict[str, Any]] = {}
```

**KullanÄ±m AkÄ±ÅŸÄ±:**

```mermaid
flowchart LR
    A["/run/{id}"] -->|Senaryo sonucu| B[LAST_EXCEL_STORE]
    B -->|scenario_id key| C["/download/{id}"]
    B -->|scenario_id key| D["/share/{id}"]
```

**Riskler:**

| Risk | Seviye | AÃ§Ä±klama |
|------|--------|----------|
| Memory Leak | ğŸ”´ YÃœKSEK | Cleanup mekanizmasÄ± YOK - Store sÃ¼resiz bÃ¼yÃ¼yor |
| Data Loss | ğŸ”´ YÃœKSEK | Server restart = TÃ¼m bekleyen indirmeler kaybedilir |
| Concurrency | ğŸŸ¡ ORTA | AynÄ± scenario_id ile paralel Ã§alÄ±ÅŸtÄ±rmalarda override |

**Optimizasyon Ã–nerileri:**
1. **TTL ekle:** 30 dakika sonra otomatik temizle
2. **LRU Cache:** En az kullanÄ±lanÄ± Ã§Ä±kar (max 100 entry)
3. **Redis geÃ§iÅŸi:** Production iÃ§in zorunlu

---

### 1.2 SHARE_STORE

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Dosya** | `backend/app/main.py` |
| **TanÄ±m SatÄ±rÄ±** | Line 461 |
| **KullanÄ±m** | Lines 528, 537, 541, 544, 562, 565, 569 |

**Kod KanÄ±tÄ±:**
```python
# main.py:461
SHARE_STORE: Dict[str, dict] = {}
SHARE_EXPIRY_SECONDS = 24 * 60 * 60  # 24 saat
```

**Cleanup MekanizmasÄ±:**
```python
# main.py:535-544 - Lazy cleanup (her POST'ta tetiklenir)
current_time = time.time()
expired_ids = [sid for sid, sdata in SHARE_STORE.items() 
               if current_time - sdata["created_at"] > SHARE_EXPIRY_SECONDS]
for sid in expired_ids:
    try:
        os.remove(SHARE_STORE[sid]["path"])  # Dosya silme
    except:
        pass
    del SHARE_STORE[sid]
```

**Riskler:**

| Risk | Seviye | AÃ§Ä±klama |
|------|--------|----------|
| Orphan Files | ğŸŸ¡ ORTA | Memory temizlendi ama disk dosyasÄ± kaldÄ± (exception) |
| Lazy Cleanup | ğŸŸ¡ ORTA | Temizlik sadece yeni share POST'unda Ã§alÄ±ÅŸÄ±r |
| File Path Exposure | ğŸŸ¢ DÃœÅÃœK | `shared_files/` dizinine doÄŸrudan eriÅŸim yok |

**Optimizasyon Ã–nerileri:**
1. **Background task** ile periyodik cleanup
2. **Startup cleanup** - Server baÅŸlangÄ±cÄ±nda eski dosyalarÄ± temizle
3. **Redis + S3** geÃ§iÅŸi production iÃ§in

---

### 1.3 SCHEDULED_JOBS

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Dosya** | `backend/app/scheduled_reports_api.py` |
| **TanÄ±m SatÄ±rÄ±** | Line 16 |
| **KullanÄ±m** | Lines 66, 86, 92, 121, 122, 131, 134, 142, 145, 146, 151, 161, 164, 177, 180, 185 |

**Kod KanÄ±tÄ±:**
```python
# scheduled_reports_api.py:14-16
# ZamanlanmÄ±ÅŸ gÃ¶revleri saklamak iÃ§in basit in-memory store
# Production'da Redis veya veritabanÄ± kullanÄ±lmalÄ±
SCHEDULED_JOBS: dict = {}
```

**API Endpoints:**

| Endpoint | Method | AÃ§Ä±klama |
|----------|--------|----------|
| `/viz/schedule/create` | POST | Yeni job oluÅŸtur |
| `/viz/schedule/list` | GET | TÃ¼m joblarÄ± listele |
| `/viz/schedule/{job_id}` | GET | Tek job detayÄ± |
| `/viz/schedule/{job_id}/toggle` | PUT | Enable/disable |
| `/viz/schedule/{job_id}` | DELETE | Job sil |
| `/viz/schedule/{job_id}/run-now` | POST | Hemen Ã§alÄ±ÅŸtÄ±r |

**Riskler:**

| Risk | Seviye | AÃ§Ä±klama |
|------|--------|----------|
| Schedule Loss | ğŸ”´ YÃœKSEK | Server restart = TÃ¼m zamanlamalar kaybolur |
| APScheduler BaÄŸÄ±mlÄ±lÄ±ÄŸÄ± | ğŸŸ¡ ORTA | ImportError durumunda job sadece kaydedilir, Ã§alÄ±ÅŸmaz |
| Duplicate Jobs | ğŸŸ¢ DÃœÅÃœK | Timestamp-based ID Ã§akÄ±ÅŸma riski dÃ¼ÅŸÃ¼k |

**Optimizasyon Ã–nerileri:**
1. **SQLite/PostgreSQL** ile kalÄ±cÄ± job storage
2. **Celery Beat** veya **APScheduler jobstore** kullanÄ±mÄ±
3. **Startup recovery** - DB'den job'larÄ± yeniden yÃ¼kle

---

## 2. /ui/inspect Endpoint Maliyeti

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Dosya** | `backend/app/ui_api.py` |
| **TanÄ±m SatÄ±rlarÄ±** | Lines 164-268 |
| **Endpoint** | `POST /ui/inspect` |

**Maliyet Analizi:**

```mermaid
flowchart TD
    A["file.read()"] -->|TÃ¼m dosya RAM'e| B["BytesIO(content)"]
    B -->|3x parse| C["df_preview (10 satÄ±r)"]
    B -->|Tekrar oku| D["df_full (TÃœM satÄ±rlar)"]
    B -->|Tekrar oku| E["raw_df (10 satÄ±r, header=None)"]
```

**Bellek KullanÄ±mÄ±:**

| Ä°ÅŸlem | Memory KullanÄ±mÄ± |
|-------|------------------|
| `content = await file.read()` | 1x dosya boyutu |
| `BytesIO(content)` - 1. read | +1x (pandas copy) |
| `BytesIO(content)` - 2. read | +1x (pandas copy) |
| `BytesIO(content)` - 3. read | +1x (pandas copy) |
| **Toplam Peak** | **~4x dosya boyutu** |

**Ã–rnek:** 100 MB Excel â†’ ~400 MB RAM gereksinimi

**Problemli Kod BloÄŸu:**
```python
# ui_api.py:187-218
content = await file.read()  # TÃœM dosya RAM'e yÃ¼klenir

# 3 AYRI PARSE:
df_preview = pd.read_excel(BytesIO(content), ..., nrows=10)  # 1. parse
df_full = pd.read_excel(BytesIO(content), ...)              # 2. parse - FULL
raw_df = pd.read_excel(BytesIO(content), header=None, nrows=10)  # 3. parse
```

---

## 3. Optimizasyon SeÃ§enekleri

### 3.1 LAST_EXCEL_STORE Ä°Ã§in

| SeÃ§enek | KarmaÅŸÄ±klÄ±k | Etkililik | Ã–neri |
|---------|-------------|-----------|-------|
| TTL + MaxSize (in-memory) | DÃ¼ÅŸÃ¼k | Orta | âœ… MVP iÃ§in uygun |
| Redis + Serialization | Orta | YÃ¼ksek | âœ… Production iÃ§in |
| Temp file + Cleanup task | DÃ¼ÅŸÃ¼k | Orta | Memory sorunlarÄ±nÄ± Ã§Ã¶zer |

**MVP Implementasyon:**
```python
# Ã–rnek TTL + MaxSize implementasyonu
import time
from collections import OrderedDict

class TTLStore:
    def __init__(self, max_size=100, ttl_seconds=1800):
        self._store = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl_seconds
    
    def set(self, key, value):
        self._cleanup()
        self._store[key] = {"data": value, "ts": time.time()}
        if len(self._store) > self.max_size:
            self._store.popitem(last=False)
    
    def get(self, key):
        item = self._store.get(key)
        if item and time.time() - item["ts"] < self.ttl:
            return item["data"]
        return None
    
    def _cleanup(self):
        now = time.time()
        expired = [k for k,v in self._store.items() if now - v["ts"] > self.ttl]
        for k in expired:
            del self._store[k]
```

---

### 3.2 /ui/inspect Ä°Ã§in

| SeÃ§enek | KarmaÅŸÄ±klÄ±k | Bellek Tasarrufu | Ã–neri |
|---------|-------------|------------------|-------|
| Streaming read | Orta | %75 | âœ… En iyi oran |
| Tek parse + tÃ¼m veriler | DÃ¼ÅŸÃ¼k | %50 | âœ… HÄ±zlÄ± fix |
| Chunk-based preview | YÃ¼ksek | %90 | Ä°leri seviye |

**Optimizasyon 1: Tek Parse**
```python
# DEÄÄ°ÅÄ°KLÄ°K: 3 parse yerine 1 parse
content = await file.read()

# TEK PARSE - TÃ¼m ihtiyaÃ§larÄ± karÅŸÄ±la
df_full = pd.read_excel(BytesIO(content), sheet_name=active_sheet, header=pandas_header)
df_preview = df_full.head(10)  # RAM'deki DF'den al

# Raw rows iÃ§in header=None ile tekrar okumak yerine:
# index-based row access kullan
raw_rows = []
for idx in range(min(10, len(df_full))):
    raw_rows.append({"cells": [str(v) if pd.notna(v) else "" for v in df_full.iloc[idx].values]})
```

**Optimizasyon 2: File Size Limit**
```python
# BÃ¼yÃ¼k dosyalar iÃ§in uyarÄ±
MAX_INSPECT_SIZE = 50 * 1024 * 1024  # 50 MB

content = await file.read()
if len(content) > MAX_INSPECT_SIZE:
    return {
        "warning": "Dosya Ã§ok bÃ¼yÃ¼k (>50MB). Preview limitli.",
        "columns": [],
        "row_count": "Unknown - dosya Ã§ok bÃ¼yÃ¼k"
    }
```

---

## 4. Risk Ã–zeti

| BileÅŸen | Risk | Ã–ncelik | Aksiyon |
|---------|------|---------|---------|
| LAST_EXCEL_STORE | Memory leak | ğŸ”´ P0 | TTL + MaxSize ekle |
| SHARE_STORE | Orphan files | ğŸŸ¡ P1 | Startup cleanup |
| SCHEDULED_JOBS | Data loss | ğŸ”´ P0 | DB persistence |
| /ui/inspect | High memory | ğŸŸ¡ P1 | Tek parse optimizasyonu |

---

## 5. Sonraki AdÄ±mlar

1. **Test PlanÄ±:** `EXCEL_STUDIO_SELFTEST_PLAN.md` - 10 kritik senaryo
2. **Golden Suite:** `EXCEL_STUDIO_GOLDEN_SUITE_PLAN.md` - 5 Ã¶rnek dataset
3. **Implementasyon:** FAZ 1'de risk azaltma

---

*Bu dokÃ¼man salt okunur denetim iÃ§in oluÅŸturulmuÅŸtur. HiÃ§bir kod deÄŸiÅŸikliÄŸi yapÄ±lmamÄ±ÅŸtÄ±r.*
